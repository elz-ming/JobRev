{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25de5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import ChunkedEncodingError\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Due to the html size is too big, converted to txt file for easy processing with BeautifulSoup\n",
    "f1  =  open(\"index.txt\" , \"r\")\n",
    "soup = BeautifulSoup(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_company_info = soup.find_all(name=\"div\", class_=\"_WfdmOZ2wLBd0x_TFyKO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = []\n",
    "company_url_list = []\n",
    "company_rating = []\n",
    "\n",
    "for i in range(len(all_company_info)):\n",
    "    company_list.append(all_company_info[i].find(name=\"a\", class_=\"_9d6_WXgVpZWn7HaXEgs\").getText())\n",
    "    \n",
    "## Get company jobstreet main url\n",
    "endpoint = \"https://www.jobstreet.com.my\"\n",
    "for i in range(len(all_company_info)):\n",
    "    company_url_list.append(endpoint + all_company_info[i].find(name=\"a\", class_=\"_9d6_WXgVpZWn7HaXEgs\")[\"href\"])\n",
    "\n",
    "\n",
    "for i in range(len(all_company_info)):\n",
    "    company_rating.append(float(all_company_info[i].find_all(name=\"span\", class_=\"j6BF91AmxWn9vS_m399j\")[1].getText().split(\"out\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert them into the basic dataframe\n",
    "data = {\n",
    "    \"company_name\" : company_list,\n",
    "    \"company_js_url\" : company_url_list,\n",
    "    \"company_rating\" : company_rating\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3eccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove company without rating\n",
    "df = df[df[\"company_rating\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf75a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of info\n",
    "five_star_list = []\n",
    "four_star_list = []\n",
    "three_star_list = []\n",
    "two_star_list = []\n",
    "one_star_list = []\n",
    "salary_rate_list = []\n",
    "recommendation_rate_list = []\n",
    "\n",
    "## To extract more information from the website\n",
    "for url in df[\"company_js_url\"]:\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    ## Info in the response\n",
    "    five_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[0].getText()\n",
    "    four_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[1].getText()\n",
    "    three_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[2].getText()\n",
    "    two_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[3].getText()\n",
    "    one_star =  soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[4].getText()\n",
    "    salary_rate = soup.find_all(name=\"div\", class_=\"RL2EfHhuwPsO8xlgwG_S\")[0].getText()\n",
    "    recommendation_rate = soup.find_all(name=\"div\", class_=\"RL2EfHhuwPsO8xlgwG_S\")[1].getText()\n",
    "\n",
    "    try:\n",
    "        salary_rate_list.append(salary_rate)\n",
    "    except:\n",
    "        salary_rate_list.append(\"No Info\")\n",
    "\n",
    "    try:\n",
    "        recommendation_rate_list.append(recommendation_rate)\n",
    "    except:\n",
    "        recommendation_rate_list.append(\"No Info\")\n",
    "\n",
    "    try:\n",
    "        five_star_list.append(five_star)\n",
    "    except:\n",
    "        five_star_list.append(\"0\")\n",
    "\n",
    "    try:\n",
    "        four_star_list.append(four_star)\n",
    "    except:\n",
    "        four_star_list.append(\"0\")\n",
    "\n",
    "    try:\n",
    "        three_star_list.append(three_star)\n",
    "    except:\n",
    "        three_star_list.append(\"0\")\n",
    "\n",
    "    try:\n",
    "        two_star_list.append(two_star)\n",
    "    except:\n",
    "        two_star_list.append(\"0\")\n",
    "\n",
    "    try:\n",
    "        one_star_list.append(one_star)\n",
    "    except:\n",
    "        one_star_list.append(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to the dataframe\n",
    "df[\"five_star_rating\"] = five_star_list\n",
    "df[\"four_star_rating\"] = four_star_list\n",
    "df[\"three_star_rating\"] = three_star_list\n",
    "df[\"two_star_rating\"] = two_star_list\n",
    "df[\"one_star_rating\"] = one_star_list\n",
    "df[\"salary_rate\"] = salary_rate_list\n",
    "df[\"recommendation_rate\"] = recommendation_rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a252ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function created to extract all the extra info from each company jobstreet website url\n",
    "## To unfilter out no info in later stage\n",
    "extra_info_list = []\n",
    "\n",
    "for i in df[\"company_js_url\"]:\n",
    "    response = requests.get(i)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        extra_info_list.append(soup.find(name=\"div\", class_=\"E5REvPGaM2o9YsLBgK75\").getText())\n",
    "    except:\n",
    "        extra_info_list.append(\"No Info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter data with No Info\n",
    "df = df[df[\"extra_info\"] != \"No Info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of lists for more info\n",
    "address_list = []\n",
    "website_list = []\n",
    "employee_num_list = []\n",
    "benefit_list = []\n",
    "dress_code_list = []\n",
    "language_list = []\n",
    "working_hours = []\n",
    "\n",
    "\n",
    "\n",
    "all_company_info_list = []\n",
    "c = 0\n",
    "\n",
    "for url in df_without_no_info[\"company_js_url\"]:\n",
    "    \n",
    "    ## Function to avoid connection peer error\n",
    "    trycnt = 3  # max try cnt\n",
    "    while trycnt > 0:\n",
    "        try:\n",
    "            result = requests.get(url)\n",
    "            c += 1\n",
    "            trycnt = 0 # success\n",
    "        except ChunkedEncodingError as ex:\n",
    "            if trycnt <= 0: print(\"Failed to retrieve: \" + url + \"\\n\" + str(ex))  # done retrying\n",
    "            else: trycnt -= 1  # retry\n",
    "            time.sleep(0.5)  # wait 1/2 second then retry\n",
    "    \n",
    "    ## Call each company website url and extract the function to single dimensional array\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    individual_company_info = []\n",
    "    cleaned_individual_company_info = []\n",
    "\n",
    "    for i in soup.find_all(name=\"div\", class_=\"kAiAILUw3ZWmw8LarQq0\"):\n",
    "        individual_company_info.append(i.getText())\n",
    "    \n",
    "    location = [s for s in individual_company_info if \"Location\" in s]\n",
    "    cleaned_individual_company_info.append(location[0] if location else \"No Info\")\n",
    "\n",
    "    website = [s for s in individual_company_info if \"Website\" in s]\n",
    "    cleaned_individual_company_info.append(website[0] if website else \"No Info\")\n",
    "\n",
    "    company_size = [s for s in individual_company_info if \"Company Size\" in s]\n",
    "    cleaned_individual_company_info.append(company_size[0] if company_size else \"No Info\")\n",
    "\n",
    "    benefit = [s for s in individual_company_info if \"Benefits\" in s]\n",
    "    cleaned_individual_company_info.append(benefit[0] if benefit else \"No Info\")\n",
    "\n",
    "    dress_code = [s for s in individual_company_info if \"Dress code\" in s]\n",
    "    cleaned_individual_company_info.append(dress_code[0] if dress_code else \"No Info\")\n",
    "\n",
    "    spoken_language = [s for s in individual_company_info if \"Spoken language\" in s]\n",
    "    cleaned_individual_company_info.append(spoken_language[0] if spoken_language else \"No Info\")\n",
    "\n",
    "    work_hour = [s for s in individual_company_info if \"Work hour\" in s]\n",
    "    cleaned_individual_company_info.append(work_hour[0] if work_hour else \"No Info\")\n",
    "\n",
    "    all_company_info_list.append(cleaned_individual_company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the collected info to list by list comprehension function\n",
    "address_list = [i[0] for i in all_company_info_list]\n",
    "website_list = [i[1] for i in all_company_info_list]\n",
    "employee_num_list = [i[2] for i in all_company_info_list]\n",
    "benefit_list = [i[3] for i in all_company_info_list]\n",
    "dress_code_list = [i[4] for i in all_company_info_list]\n",
    "language_list = [i[5] for i in all_company_info_list]\n",
    "working_hours = [i[6] for i in all_company_info_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the extra info column which we no longer need to use\n",
    "df = df.drop(\"extra_info\", index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3883f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Futher cleaning on the dataset\n",
    "df[\"company_address\"] = df[\"company_address\"].apply(lambda x:x.replace(\"Location\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_website\"] = df[\"company_website\"].apply(lambda x:x.replace(\"Website\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_employee_num\"] = df[\"company_employee_num\"].apply(lambda x:x.replace(\"Company Size\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_benefit\"] = df[\"company_benefit\"].apply(lambda x:x.replace(\"Benefits\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_dress_code\"] = df[\"company_dress_code\"].apply(lambda x:x.replace(\"Dress code\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_language\"] = df[\"company_language\"].apply(lambda x:x.replace(\"Spoken language\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_work_hours\"] = df[\"company_work_hours\"].apply(lambda x:x.replace(\"Work hours\", \"\").replace('\\n', ' ').replace('\\r', '').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef55a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to CSV\n",
    "df.to_csv(\"company_list_with_info.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
