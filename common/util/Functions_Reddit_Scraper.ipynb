{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('DataFile/Reddit_Top_Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac551e",
   "metadata": {},
   "source": [
    "# Function#1 - Scraping No.1 Hot Submission Info in r/jobs :\n",
    "- Subreddit ID\n",
    "- Subreddit score\n",
    "- Subreddit comment numbers\n",
    "- Subreddit title\n",
    "- Subreddit body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmInfo():\n",
    "    import praw\n",
    "    import pandas as pd\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"bfx8ExsQXjGUptX6jLA5Rg\",\n",
    "        client_secret=\"fwnSOPiCXFTv4JtKSX9iqLd3r40YcA\",\n",
    "        user_agent=\"Scraper1\",\n",
    "    )\n",
    "\n",
    "    hot_posts = reddit.subreddit('Jobs').hot()\n",
    "\n",
    "    post_list = []\n",
    "\n",
    "    for post in hot_posts:\n",
    "        post_list.append([post.id, post.score, post.num_comments, post.title, post.selftext])\n",
    "\n",
    "\n",
    "    post_df = pd.DataFrame(post_list, columns=['id','score','comm_num','title','body'])\n",
    "\n",
    "    post_df = post_df.sort_values('score',ascending=False).reset_index()\n",
    "    post_df.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    top_subm_dict = post_df.iloc[0].to_dict()\n",
    "\n",
    "    return top_subm_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d4f83",
   "metadata": {},
   "source": [
    "# Function#2 - Getting the top comments + Extensive cleaning :\n",
    "- Lower-casing\n",
    "- Tokenization + Removing punctuations\n",
    "- Removing stopwords\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopComm(TopSubmInfoDF):\n",
    "    import praw\n",
    "    from praw.models import MoreComments\n",
    "\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "    import string\n",
    "\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "            client_id=\"bfx8ExsQXjGUptX6jLA5Rg\",\n",
    "            client_secret=\"fwnSOPiCXFTv4JtKSX9iqLd3r40YcA\",\n",
    "            user_agent=\"Scraper1\",\n",
    "            )\n",
    "\n",
    "    top_id = TopSubmInfoDF['id']\n",
    "    submission = reddit.submission(id=top_id)\n",
    "\n",
    "    comment_list = []\n",
    "\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comm in submission.comments:\n",
    "        \n",
    "        #1 - Lower-casing\n",
    "        comment = comm.body.lower()\n",
    "\n",
    "        #2 - Tokenize and removing punctuations\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "        clean = tokenizer.tokenize(comment)\n",
    "\n",
    "        #3 - Removing stopwords and singular characters\n",
    "        stopword = stopwords.words('english')\n",
    "        stopword = stopword + list(string.ascii_lowercase)\n",
    "        word_list = []\n",
    "        for word in clean:\n",
    "            if word not in stopword:\n",
    "                word_list.append(word)\n",
    "\n",
    "        #4 - Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for w in word_list:\n",
    "            comment_list.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    return comment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af94440",
   "metadata": {},
   "source": [
    "### Mini Checkpoint : Run Scrapers and return CommList for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = rscraper2(rscraper1())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75f651",
   "metadata": {},
   "source": [
    "# Function#3 - Printing Word Frequency Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rplot1(comment_list):\n",
    "    from nltk.probability import FreqDist\n",
    "    import matplotlib as pl\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fdist=FreqDist(comment_list)\n",
    "    fdist.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df7281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rplot1(comment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390dd70",
   "metadata": {},
   "source": [
    "# Fucntion#4 - Printing WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rplot2(comment_list):\n",
    "    from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "    import matplotlib as pl\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    from PIL import Image\n",
    "    \n",
    "    # Because WordCloud takes in string but not list\n",
    "    comment_string = ' '.join(comment_list)\n",
    "    \n",
    "    # Mask with a RM50 bill\n",
    "    mask = np.array(Image.open('./RM100.jpg')) \n",
    "    color= ImageColorGenerator(mask)\n",
    "    \n",
    "    # WordCloud settings\n",
    "    wordcloud=WordCloud(\n",
    "        background_color='white', \n",
    "        max_font_size=200,\n",
    "        max_words=1000,\n",
    "        mask=mask,\n",
    "        random_state=42,\n",
    "        repeat=True,\n",
    "        )\n",
    "    \n",
    "    # WordCloud generator\n",
    "    wordcloud.generate(comment_string)\n",
    "    \n",
    "    # WordCloud plotter\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud.recolor(color_func=color), interpolation=\"bilinear\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06615d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rplot2(comment_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b3492",
   "metadata": {},
   "source": [
    "# Function#5 - A repeating cron job tasked everyday to scrape new info :\n",
    "- If the new top hot submission has a higher score than previous, output a new wordcloud\n",
    "- If the existing top hot submission is still the higher score, update the word counts and reprint wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea719a",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df51a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT_ANTS#\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "import os.path\n",
    "\n",
    "#INITIATING REDDIT\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"bfx8ExsQXjGUptX6jLA5Rg\",\n",
    "    client_secret=\"fwnSOPiCXFTv4JtKSX9iqLd3r40YcA\",\n",
    "    user_agent=\"Scraper1\",\n",
    ")\n",
    "\n",
    "#GETTING HOT POST\n",
    "hot_posts = reddit.subreddit('Jobs').hot()\n",
    "\n",
    "post_list = []\n",
    "\n",
    "for post in hot_posts:\n",
    "    post_list.append([post.id, post.score, post.num_comments, post.title, post.selftext])\n",
    "\n",
    "\n",
    "post_df = pd.DataFrame(post_list, columns=['id','score','comm_num','title','body'])\n",
    "\n",
    "post_df = post_df.sort_values('score',ascending=False).reset_index()\n",
    "post_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "top_subm_id = post_df['id'][0]\n",
    "submission = reddit.submission(id=top_subm_id)\n",
    "\n",
    "word_list = []\n",
    "sent_list = []\n",
    "\n",
    "submission.comments.replace_more(limit=0)\n",
    "for comm in submission.comments:\n",
    "    print(comm.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f336c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#IMPORT_ANTS#\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "import os.path\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "#INITIATING REDDIT\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"bfx8ExsQXjGUptX6jLA5Rg\",\n",
    "    client_secret=\"fwnSOPiCXFTv4JtKSX9iqLd3r40YcA\",\n",
    "    user_agent=\"Scraper1\",\n",
    ")\n",
    "\n",
    "#GETTING HOT POST\n",
    "hot_posts = reddit.subreddit('Jobs').hot()\n",
    "\n",
    "post_list = []\n",
    "\n",
    "for post in hot_posts:\n",
    "    post_list.append([post.id, post.score, post.num_comments, post.title, post.selftext])\n",
    "\n",
    "\n",
    "post_df = pd.DataFrame(post_list, columns=['id','score','comm_num','title','body'])\n",
    "\n",
    "post_df = post_df.sort_values('score',ascending=False).reset_index()\n",
    "post_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "top_subm_id = post_df['id'][0]\n",
    "submission = reddit.submission(id=top_subm_id)\n",
    "\n",
    "word_list = []\n",
    "sent_list = []\n",
    "\n",
    "submission.comments.replace_more(limit=0)\n",
    "for comm in submission.comments:\n",
    "    \n",
    "    #1 - Lower-casing\n",
    "    comment = comm.body.lower()\n",
    "    \n",
    "    #2 - Tokenize and removing punctuations\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    clean_word = tokenizer.tokenize(comment)\n",
    "    \n",
    "    #3 - Removing singular characters and returning sentence\n",
    "    words_wout_alphabets = []\n",
    "    for w in clean_word:\n",
    "        if w not in list(string.ascii_lowercase):\n",
    "            words_wout_alphabets.append(w)\n",
    "    clean_sent = ' '.join(words_wout_alphabets)\n",
    "    sent_list.append(clean_sent)\n",
    "    \n",
    "    #4 - Removing stopwords\n",
    "    stopword = stopwords.words('english')\n",
    "    words_wout_stopwords = []\n",
    "    for w in words_wout_alphabets:\n",
    "        if w not in stopword:\n",
    "            words_wout_stopwords.append(w)\n",
    "    \n",
    "    #5 - Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in words_wout_stopwords:\n",
    "        word_list.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "Subm_Today = {\n",
    "    \"date\" : str(date.today()),\n",
    "    \"id\" : post_df['id'][0],\n",
    "    \"score\" : int(post_df['score'][0]),\n",
    "    \"title\" : post_df['title'][0],\n",
    "    \"body\" : post_df['body'][0],\n",
    "    \"sent_num\" : len(sent_list),\n",
    "    \"sent_str\" : str(sent_list),\n",
    "    \"word_num\" : len(word_list),\n",
    "    \"u_word_num\" : len(set(word_list)),\n",
    "    \"word_str\" : str(word_list),\n",
    "}\n",
    "\n",
    "to_save = pd.DataFrame(Subm_Today,index=[0])\n",
    "\n",
    "if  os.path.isfile('DataFile/Reddit_Top_Submission.csv'):\n",
    "    df = pd.read_csv('DataFile/Reddit_Top_Submission.csv')\n",
    "    df_updated = pd.concat([df,to_save])\n",
    "else:\n",
    "    df_updated = to_save\n",
    "df_updated.to_csv('DataFile/Reddit_Top_Submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e85a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
