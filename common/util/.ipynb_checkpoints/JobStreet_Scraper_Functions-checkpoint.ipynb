{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6688b0cd",
   "metadata": {},
   "source": [
    "## FUNCTION#1 - Using Selenium to open website and BeautifulSoup to scrape :\n",
    "- Company Name\n",
    "- Company JSON Url\n",
    "- Company Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8218334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jsscraper1():\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.jobstreet.com.my/en/companies/browse-reviews'\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "while counter < 100:\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, 'Gk2xjdpX_s0S3hsfi__X').click()\n",
    "        counter += 1\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            driver.find_element(By.CLASS_NAME, 'gwCwc3YZYufjgIcBtu0l').click()\n",
    "        except:\n",
    "            continue\n",
    "    except:\n",
    "        break\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "all_company_info = soup.find_all(name=\"div\", class_=\"_WfdmOZ2wLBd0x_TFyKO\")\n",
    "\n",
    "company_list = []\n",
    "company_url_list = []\n",
    "company_rating = []\n",
    "\n",
    "for i in range(len(all_company_info)):\n",
    "    company_list.append(all_company_info[i].find(name=\"a\", class_=\"_9d6_WXgVpZWn7HaXEgs\").getText())\n",
    "\n",
    "## Get company jobstreet main url\n",
    "endpoint = \"https://www.jobstreet.com.my\"\n",
    "for i in range(len(all_company_info)):\n",
    "    company_url_list.append(endpoint + all_company_info[i].find(name=\"a\", class_=\"_9d6_WXgVpZWn7HaXEgs\")[\"href\"])\n",
    "\n",
    "\n",
    "for i in range(len(all_company_info)):\n",
    "    company_rating.append(float(all_company_info[i].find_all(name=\"span\", class_=\"j6BF91AmxWn9vS_m399j\")[1].getText().split(\"out\")[0]))\n",
    "\n",
    "data = {\n",
    "    \"company_name\" : company_list,\n",
    "    \"company_js_url\" : company_url_list,\n",
    "    \"company_rating\" : company_rating\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=data)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e5e7e",
   "metadata": {},
   "source": [
    "## FUNCTION#2 - Pass urls in dataframe to BeautifulSoup and retrieve :\n",
    "\n",
    "- Salary Rate\n",
    "- Recommendation Rate\n",
    "- 5 Star Rate\n",
    "- 4 Star Rate\n",
    "- 3 Star Rate\n",
    "- 2 Star Rate\n",
    "- 1 Star Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe4c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def jsscraper2(Dataframe):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "\n",
    "    df = Dataframe\n",
    "\n",
    "    five_star_list = []\n",
    "    four_star_list = []\n",
    "    three_star_list = []\n",
    "    two_star_list = []\n",
    "    one_star_list = []\n",
    "    salary_rate_list = []\n",
    "    recommendation_rate_list = []\n",
    "\n",
    "    for url in df['company_js_url']:\n",
    "        response = requests.get(url)\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            salary_rate = soup.find_all(name=\"div\", class_=\"RL2EfHhuwPsO8xlgwG_S\")[0].getText()\n",
    "            salary_rate_list.append(salary_rate)\n",
    "        except:\n",
    "            salary_rate_list.append(\"No Info\")\n",
    "\n",
    "        try:\n",
    "            recommendation_rate = soup.find_all(name=\"div\", class_=\"RL2EfHhuwPsO8xlgwG_S\")[1].getText()\n",
    "            recommendation_rate_list.append(recommendation_rate)\n",
    "        except:\n",
    "            recommendation_rate_list.append(\"No Info\")\n",
    "\n",
    "        try:\n",
    "            five_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[0].getText()\n",
    "            five_star_list.append(five_star)\n",
    "        except:\n",
    "            five_star_list.append(\"0\")\n",
    "\n",
    "        try:\n",
    "            four_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[1].getText()\n",
    "            four_star_list.append(four_star)\n",
    "        except:\n",
    "            four_star_list.append(\"0\")\n",
    "\n",
    "        try:\n",
    "            three_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[2].getText()\n",
    "            three_star_list.append(three_star)\n",
    "        except:\n",
    "            three_star_list.append(\"0\")\n",
    "\n",
    "        try:\n",
    "            two_star = soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[3].getText()\n",
    "            two_star_list.append(two_star)\n",
    "        except:\n",
    "            two_star_list.append(\"0\")\n",
    "\n",
    "        try:\n",
    "            one_star =  soup.find_all(name=\"span\", class_=\"DCaw56gNq5O1IBoC5xcz\")[4].getText()\n",
    "            one_star_list.append(one_star)\n",
    "        except:\n",
    "            one_star_list.append(\"0\")\n",
    "\n",
    "    df[\"five_star_rating\"] = five_star_list\n",
    "    df[\"four_star_rating\"] = four_star_list\n",
    "    df[\"three_star_rating\"] = three_star_list\n",
    "    df[\"two_star_rating\"] = two_star_list\n",
    "    df[\"one_star_rating\"] = one_star_list\n",
    "    df[\"salary_rate\"] = salary_rate_list\n",
    "    df[\"recommendation_rate\"] = recommendation_rate_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11481eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = jsscraper1()\n",
    "df = jsscraper2(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b5f35",
   "metadata": {},
   "source": [
    "## FUNCTION#3 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a252ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function created to extract all the extra info from each company jobstreet website url\n",
    "## To unfilter out no info in later stage\n",
    "extra_info_list = []\n",
    "\n",
    "for i in df[\"company_js_url\"]:\n",
    "    response = requests.get(i)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        extra_info_list.append(soup.find(name=\"div\", class_=\"E5REvPGaM2o9YsLBgK75\").getText())\n",
    "    except:\n",
    "        extra_info_list.append(\"No Info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter data with No Info\n",
    "df = df[df[\"extra_info\"] != \"No Info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of lists for more info\n",
    "address_list = []\n",
    "website_list = []\n",
    "employee_num_list = []\n",
    "benefit_list = []\n",
    "dress_code_list = []\n",
    "language_list = []\n",
    "working_hours = []\n",
    "\n",
    "\n",
    "\n",
    "all_company_info_list = []\n",
    "c = 0\n",
    "\n",
    "for url in df_without_no_info[\"company_js_url\"]:\n",
    "    \n",
    "    ## Function to avoid connection peer error\n",
    "    trycnt = 3  # max try cnt\n",
    "    while trycnt > 0:\n",
    "        try:\n",
    "            result = requests.get(url)\n",
    "            c += 1\n",
    "            trycnt = 0 # success\n",
    "        except ChunkedEncodingError as ex:\n",
    "            if trycnt <= 0: print(\"Failed to retrieve: \" + url + \"\\n\" + str(ex))  # done retrying\n",
    "            else: trycnt -= 1  # retry\n",
    "            time.sleep(0.5)  # wait 1/2 second then retry\n",
    "    \n",
    "    ## Call each company website url and extract the function to single dimensional array\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    individual_company_info = []\n",
    "    cleaned_individual_company_info = []\n",
    "\n",
    "    for i in soup.find_all(name=\"div\", class_=\"kAiAILUw3ZWmw8LarQq0\"):\n",
    "        individual_company_info.append(i.getText())\n",
    "    \n",
    "    location = [s for s in individual_company_info if \"Location\" in s]\n",
    "    cleaned_individual_company_info.append(location[0] if location else \"No Info\")\n",
    "\n",
    "    website = [s for s in individual_company_info if \"Website\" in s]\n",
    "    cleaned_individual_company_info.append(website[0] if website else \"No Info\")\n",
    "\n",
    "    company_size = [s for s in individual_company_info if \"Company Size\" in s]\n",
    "    cleaned_individual_company_info.append(company_size[0] if company_size else \"No Info\")\n",
    "\n",
    "    benefit = [s for s in individual_company_info if \"Benefits\" in s]\n",
    "    cleaned_individual_company_info.append(benefit[0] if benefit else \"No Info\")\n",
    "\n",
    "    dress_code = [s for s in individual_company_info if \"Dress code\" in s]\n",
    "    cleaned_individual_company_info.append(dress_code[0] if dress_code else \"No Info\")\n",
    "\n",
    "    spoken_language = [s for s in individual_company_info if \"Spoken language\" in s]\n",
    "    cleaned_individual_company_info.append(spoken_language[0] if spoken_language else \"No Info\")\n",
    "\n",
    "    work_hour = [s for s in individual_company_info if \"Work hour\" in s]\n",
    "    cleaned_individual_company_info.append(work_hour[0] if work_hour else \"No Info\")\n",
    "\n",
    "    all_company_info_list.append(cleaned_individual_company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the collected info to list by list comprehension function\n",
    "address_list = [i[0] for i in all_company_info_list]\n",
    "website_list = [i[1] for i in all_company_info_list]\n",
    "employee_num_list = [i[2] for i in all_company_info_list]\n",
    "benefit_list = [i[3] for i in all_company_info_list]\n",
    "dress_code_list = [i[4] for i in all_company_info_list]\n",
    "language_list = [i[5] for i in all_company_info_list]\n",
    "working_hours = [i[6] for i in all_company_info_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the extra info column which we no longer need to use\n",
    "df = df.drop(\"extra_info\", index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3883f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Futher cleaning on the dataset\n",
    "df[\"company_address\"] = df[\"company_address\"].apply(lambda x:x.replace(\"Location\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_website\"] = df[\"company_website\"].apply(lambda x:x.replace(\"Website\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_employee_num\"] = df[\"company_employee_num\"].apply(lambda x:x.replace(\"Company Size\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_benefit\"] = df[\"company_benefit\"].apply(lambda x:x.replace(\"Benefits\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_dress_code\"] = df[\"company_dress_code\"].apply(lambda x:x.replace(\"Dress code\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_language\"] = df[\"company_language\"].apply(lambda x:x.replace(\"Spoken language\", \"\").replace('\\n', ' ').replace('\\r', '').strip())\n",
    "df[\"company_work_hours\"] = df[\"company_work_hours\"].apply(lambda x:x.replace(\"Work hours\", \"\").replace('\\n', ' ').replace('\\r', '').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef55a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to CSV\n",
    "df.to_csv(\"company_list_with_info.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
